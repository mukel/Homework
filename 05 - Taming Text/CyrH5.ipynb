{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA / Applied Data Analysis\n",
    "<h2 style=\"color:#a8a8a8\">Homework 5 - Taming text<br>\n",
    "Aimée Montero, Alfonso Peterssen, Cyriaque Brousse</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "In this homework you will explore a relatively large corpus of emails released in public during the\n",
    "[Hillary Clinton email controversy](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy).\n",
    "You can find the corpus in the `hillary-clinton-emails` directory of this repository, while more detailed information \n",
    "about the [schema is available here](https://www.kaggle.com/kaggle/hillary-clinton-emails).\n",
    "\n",
    "## Assignment\n",
    "1. Generate a word cloud based on the raw corpus -- I recommend you to use the [Python word_cloud library](https://github.com/amueller/word_cloud).\n",
    "With the help of `nltk` (already available in your Anaconda environment), implement a standard text pre-processing \n",
    "pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and\n",
    "cons (if any) of the two word clouds you generated.<br><br>\n",
    "\n",
    "2. Find all the mentions of world countries in the whole corpus, using the `pycountry` utility (*HINT*: remember that\n",
    "there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.)\n",
    "Perform sentiment analysis on every email message using the demo methods in the `nltk.sentiment.util` module. Aggregate \n",
    "the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level)\n",
    "that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo\n",
    "methods from the sentiment analysis module -- can you find substantial differences?<br><br>\n",
    "\n",
    "3. Using the `models.ldamodel` module from the [gensim library](https://radimrehurek.com/gensim/index.html), run topic\n",
    "modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which\n",
    "returns topics that you consider to be meaningful at first sight.<br><br>\n",
    "\n",
    "4. *BONUS*: build the communication graph (unweighted and undirected) among the different email senders and recipients\n",
    "using the `NetworkX` library. Find communities in this graph with `community.best_partition(G)` method from the \n",
    "[community detection module](http://perso.crans.org/aynaud/communities/index.html). Print the most frequent 20 words used\n",
    "by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA?\n",
    "Can you identify clear discussion topics for each community? Discuss briefly the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the required libraries for the homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./hillary-clinton-emails/Emails.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep only the fields that contain text data. That is, `ExtractedSubject` and `ExtractedBodyText`.<br>\n",
    "We drop the `NA` values, since they contain no text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_mails = ' '.join(list(data.ExtractedSubject.dropna())\n",
    "                   + list(data.ExtractedBodyText.dropna()))\n",
    "raw_mails[:130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First wordcloud attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display a first wordcloud without doing any preprocessing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud().generate(raw_mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first problem is that the cloud contains many parasite tokens that are not words that bring much information to us: for instance, \"Fw\", \"Re\", \"pm\", \"am\", etc.<br>\n",
    "We also notice that there are also actual words like \"new\", \"call\", \"one\", that don't bring any information either. These so-called **stopwords** need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, `raw_mails` is just a long string where every email and its subject is concatenated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(raw_mails), len(raw_mails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is to tokenize this string into separate words. This is done with a **tokenizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw_mails)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowerization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform all the tokens to lowercase, with the notable exception of `US` (the country) that risks being matched to `us` (the pronoun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowercase_tokens = [w.lower() for w in tokens if w not in ['US', 'U.S.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the stopwords, we use a **stopword dictionary** provided by the NLTK API.<br>\n",
    "Additionally, we remove the punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_sw = ['pm', 'am', 're', 'fw', 'fvv', '…', 'n\\'t']\n",
    "stop = stopwords.words('english') + punctuation + custom_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_tokens = list(filter(lambda w: w not in stop, lowercase_tokens))\n",
    "filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now trying to map each token to its stem. For example:\n",
    "- `reading => read`\n",
    "- `reader => read`\n",
    "- `read => read`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "stems = [porter.stem(token) for token in filtered_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(' '.join(stems))\n",
    "plt.subplots(figsize=(10,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem now, is that we have words that are not actual words. For exemple, `secretary` was stemmed to `secretari`, which doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same concept of reducing each token to a more general form as in stemming. However, we will reduce to the root form of the token, not to its stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=40).generate(' '.join(lemmas))\n",
    "plt.subplots(figsize=(10,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is much more satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pycountry.countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_names = [ x.name.lower() for x in pycountry.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_codes = [[lower_case(x.alpha_2), lower_case(x.alpha_3)] for x in pycountry.countries]\n",
    "country_codes = [item for country_code_sublist in country_codes for item in country_code_sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_dic_list = [ {lower_case(x.name)    :x.name.lower(),\n",
    "                      lower_case(x.alpha_2) :x.name.lower(), \n",
    "                      lower_case(x.alpha_3) :x.name.lower()} for x in pycountry.countries ]\n",
    "\n",
    "country_dic = {k: v for dic in country_dic_list for k, v in dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [item for item in lemmatised_words if item in country_dic ]\n",
    "len(test) / len(lemmatised_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries = [item if item not in country_dic else country_dic.get(item) for item in lemmatised_words]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
